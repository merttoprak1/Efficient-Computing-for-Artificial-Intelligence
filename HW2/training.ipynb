{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "2ae162be845142f8b15d322f948161f5",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "eEEI17m_M5uW"
      },
      "source": [
        "### Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "559c23a50e16493e99c8fcf4e1e0170f",
        "deepnote_cell_type": "code",
        "id": "mOb3TWhjM5uX"
      },
      "outputs": [],
      "source": [
        "# Using \"content\" in google colab, \"work\" in deepnote\n",
        "!cp /content/msc-train.zip /tmp/msc-train.zip\n",
        "!cp /content/msc-val.zip /tmp/msc-val.zip\n",
        "!cp /content/msc-test.zip /tmp/msc-test.zip\n",
        "!unzip -oqqq /content/msc-train.zip -d /tmp/msc-train\n",
        "!unzip -oqqq /content/msc-val.zip -d /tmp/msc-val\n",
        "!unzip -oqqq /content/msc-test.zip -d /tmp/msc-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "eec663de5c4f453bb4a1504ac7c3265d",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "ZkKjeuvhM5uX"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cell_id": "b0bc2ffa92644f08b4e1fdd5bfb6bd17",
        "deepnote_cell_type": "code",
        "execution_context_id": "5a0b763e-8f84-45e7-80a9-b4378bb88e17",
        "execution_millis": 2,
        "execution_start": 1767783120347,
        "id": "Cykx2cEkM5uX",
        "source_hash": "33c020a0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import copy\n",
        "import onnx\n",
        "import wave\n",
        "import shutil\n",
        "import time\n",
        "import onnxruntime as ort\n",
        "from onnxruntime.quantization import (\n",
        "    CalibrationDataReader, CalibrationMethod, QuantFormat,\n",
        "    QuantType, StaticQuantConfig, quantize\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0900a79aa93a45af9aa1ebb2494687fd",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "q5vQscHIM5uY"
      },
      "source": [
        "### Configuration & Reproducibility Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cell_id": "0e55ed87b8e84c6cb67d3fb0e3b36f5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "execution_context_id": "5a0b763e-8f84-45e7-80a9-b4378bb88e17",
        "execution_millis": 2,
        "execution_start": 1767783144348,
        "id": "WqUdAMBcM5uY",
        "outputId": "c52e0fda-7191-4aae-970e-c853d4b29db8",
        "source_hash": "46b7c1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Device: cuda\n"
          ]
        }
      ],
      "source": [
        "CFG = {\n",
        "    'seed': 42,\n",
        "    'sampling_rate': 16000,\n",
        "    # Feature Extraction\n",
        "    'n_fft': 512,\n",
        "    'win_length': 400,   # 25ms window\n",
        "    'hop_length': 320,\n",
        "    'n_mels': 46,        # High frequency resolution to balance time resolution loss\n",
        "\n",
        "    # Training\n",
        "    'batch_size': 32,\n",
        "    'lr': 0.005,\n",
        "    'epochs': 100,\n",
        "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "}\n",
        "\n",
        "def set_seed(seed=CFG['seed']):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed()\n",
        "print(f\"[INFO] Device: {CFG['device']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "75b84b5829124a889e42d75880e09ba0",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "RaCfZ1N1M5uZ"
      },
      "source": [
        "### Dataset Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cell_id": "5d51bbda5050420eb97972d714e8e6ec",
        "deepnote_cell_type": "code",
        "execution_context_id": "5a0b763e-8f84-45e7-80a9-b4378bb88e17",
        "execution_millis": 1,
        "execution_start": 1767783145855,
        "id": "eI9VBm9fM5uZ",
        "source_hash": "e4307f4a"
      },
      "outputs": [],
      "source": [
        "class MSCDataset(Dataset):\n",
        "    def __init__(self, data_dir, subset, wanted_words=None):\n",
        "        self.subset = subset\n",
        "        self.data_path = Path(data_dir)\n",
        "        self.wanted_words = wanted_words\n",
        "        if not self.data_path.exists():\n",
        "            # Fallback or silent fail to avoid crash if path doesn't exist locally\n",
        "            print(f\"[WARNING] Directory not found: {self.data_path}\")\n",
        "            self.file_paths = []\n",
        "        else:\n",
        "            print(f\"[INFO] Scanning {self.subset} set in {self.data_path}...\")\n",
        "            all_paths = sorted(list(self.data_path.glob(\"**/*.wav\")))\n",
        "            self.file_paths = []\n",
        "            for p in all_paths:\n",
        "                if p.name.startswith(\"._\"): continue\n",
        "                parts = p.name.split('_')\n",
        "                if len(parts) > 0:\n",
        "                    label = parts[0]\n",
        "                    if self.wanted_words is None or label in self.wanted_words:\n",
        "                        self.file_paths.append(p)\n",
        "\n",
        "        if len(self.file_paths) > 0:\n",
        "            self.labels_str = [p.name.split('_')[0] for p in self.file_paths]\n",
        "            self.classes = sorted(set(self.labels_str))\n",
        "            self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
        "            self.labels = [self.class_to_idx[lbl] for lbl in self.labels_str]\n",
        "            print(f\"[INFO] Loaded {self.subset}: {len(self.file_paths)} files. Caching...\")\n",
        "            self.cached_wavs = [self.load_wav(str(p)) for p in self.file_paths]\n",
        "        else:\n",
        "            self.cached_wavs = []\n",
        "            self.labels = []\n",
        "            print(f\"[WARNING] No files found for {self.subset}\")\n",
        "\n",
        "    def load_wav(self, wav_path):\n",
        "        try:\n",
        "            with wave.open(wav_path, 'rb') as wf:\n",
        "                sr, n_frames = wf.getframerate(), wf.getnframes()\n",
        "                n_channels, width = wf.getnchannels(), wf.getsampwidth()\n",
        "                raw_bytes = wf.readframes(n_frames)\n",
        "            if width == 2: data = np.frombuffer(raw_bytes, dtype=np.int16).astype(np.float32) / 32768.0\n",
        "            elif width == 1: data = (np.frombuffer(raw_bytes, dtype=np.uint8).astype(np.float32) - 128.0) / 128.0\n",
        "            else: data = np.frombuffer(raw_bytes, dtype=np.int32).astype(np.float32) / 2147483648.0\n",
        "            if n_channels > 1: data = data.reshape(-1, n_channels)\n",
        "            waveform = torch.from_numpy(data).float()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {wav_path}: {e}\")\n",
        "            return torch.zeros(1, CFG[\"sampling_rate\"])\n",
        "\n",
        "        if waveform.dim() == 1: waveform = waveform.unsqueeze(0)\n",
        "        else: waveform = waveform.t()\n",
        "\n",
        "        # Resample logic\n",
        "        if sr != CFG[\"sampling_rate\"]:\n",
        "            waveform = T.Resample(sr, CFG[\"sampling_rate\"])(waveform)\n",
        "\n",
        "        if waveform.shape[1] > CFG[\"sampling_rate\"]: waveform = waveform[:, :CFG[\"sampling_rate\"]]\n",
        "        elif waveform.shape[1] < CFG[\"sampling_rate\"]:\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, CFG[\"sampling_rate\"] - waveform.shape[1]))\n",
        "        return waveform\n",
        "\n",
        "    def __len__(self): return len(self.file_paths)\n",
        "    def __getitem__(self, idx): return self.cached_wavs[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "02fc516e1db94bc59c3a307a69039e10",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "fgdogE0EM5uZ"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cell_id": "cab4096c9d0b46beb7bc367e9802d1da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "execution_context_id": "5a0b763e-8f84-45e7-80a9-b4378bb88e17",
        "execution_millis": 280,
        "execution_start": 1767783146416,
        "id": "ciKfgiUEM5uZ",
        "outputId": "94b51afe-a44a-4e23-bc5f-60d8f217f927",
        "source_hash": "24cc85a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Scanning train set in /tmp/msc-train...\n",
            "[INFO] Loaded train: 1600 files. Caching...\n",
            "[INFO] Scanning val set in /tmp/msc-val...\n",
            "[INFO] Loaded val: 200 files. Caching...\n",
            "[INFO] Scanning test set in /tmp/msc-test...\n",
            "[INFO] Loaded test: 200 files. Caching...\n"
          ]
        }
      ],
      "source": [
        "CLASSES = ['stop', 'up']\n",
        "# Paths\n",
        "train_ds = MSCDataset('/tmp/msc-train', subset='train', wanted_words=CLASSES)\n",
        "val_ds   = MSCDataset('/tmp/msc-val',   subset='val',   wanted_words=CLASSES)\n",
        "test_ds  = MSCDataset('/tmp/msc-test',  subset='test',  wanted_words=CLASSES)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=CFG['batch_size'], shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_ds, batch_size=CFG['batch_size'], shuffle=False, num_workers=0)\n",
        "test_loader  = DataLoader(test_ds, batch_size=CFG['batch_size'], shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "112993462e234059a578d3bb8b605296",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "gukyoCwTM5uZ"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cell_id": "6cbd1c9938dc411bbcf3588df6b0a2da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "execution_context_id": "5a0b763e-8f84-45e7-80a9-b4378bb88e17",
        "execution_millis": 4,
        "execution_start": 1767783147146,
        "id": "5lXqZlmZM5uZ",
        "outputId": "d5a86d4a-f9ea-46bc-a1df-5d595e41ddf8",
        "source_hash": "843dd42b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Number of Model Parameters: 15,150\n"
          ]
        }
      ],
      "source": [
        "# Frontend:\n",
        "class LogMelSpec(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.melspec = T.MelSpectrogram(\n",
        "            sample_rate=CFG['sampling_rate'],\n",
        "            n_fft=CFG['n_fft'],\n",
        "            win_length=CFG['win_length'],\n",
        "            hop_length=CFG['hop_length'],\n",
        "            n_mels=CFG['n_mels']\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, 1, T)\n",
        "        mel = self.melspec(x)\n",
        "        # Log compression\n",
        "        return torch.log(mel + 1e-6)\n",
        "\n",
        "# Augmentation:\n",
        "class AddGaussianNoise(nn.Module):\n",
        "    def __init__(self, p=0.5, min_amp=0.001, max_amp=0.015):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.min_amp = min_amp\n",
        "        self.max_amp = max_amp\n",
        "    def forward(self, x):\n",
        "        if self.training and random.random() < self.p:\n",
        "            noise = torch.randn_like(x) * random.uniform(self.min_amp, self.max_amp)\n",
        "            return x + noise\n",
        "        return x\n",
        "\n",
        "class RandomGain(nn.Module):\n",
        "    def __init__(self, pb=0.5, min_gain=0.8, max_gain=1.2):\n",
        "        super().__init__()\n",
        "        self.pb = pb\n",
        "        self.min_gain = min_gain\n",
        "        self.max_gain = max_gain\n",
        "    def forward(self, x):\n",
        "        if self.training and random.random() < self.pb:\n",
        "            return x * random.uniform(self.min_gain, self.max_gain)\n",
        "        return x\n",
        "\n",
        "class RandomShift(nn.Module):\n",
        "    def __init__(self, p=0.5, max_shift=1600):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.max_shift = max_shift\n",
        "    def forward(self, x):\n",
        "        if self.training and random.random() < self.p:\n",
        "            shift = random.randint(-self.max_shift, self.max_shift)\n",
        "            return torch.roll(x, shift, dims=-1)\n",
        "        return x\n",
        "\n",
        "augment = nn.Sequential(\n",
        "    RandomShift(p=0.6),\n",
        "    RandomGain(pb=0.5),\n",
        "    AddGaussianNoise(p=0.4)\n",
        ")\n",
        "\n",
        "# Backend: DSCNN\n",
        "class DSCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        # Base=28 for lower latency on RPi\n",
        "        base = 28\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, base, kernel_size=3, stride=(2, 1), padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(base)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            self._ds_block(base, base, 1),\n",
        "            self._ds_block(base, base*2, 2),\n",
        "            self._ds_block(base*2, base*2, 1),\n",
        "            self._ds_block(base*2, base*2, 1),\n",
        "            self._ds_block(base*2, base*2, 1)\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(base*2, num_classes)\n",
        "\n",
        "    @property\n",
        "    def backbone(self):\n",
        "        return self\n",
        "\n",
        "    def _ds_block(self, in_ch, out_ch, stride):\n",
        "        return nn.Sequential(\n",
        "            # Depthwise\n",
        "            nn.Conv2d(in_ch, in_ch, 3, stride=stride, padding=1, groups=in_ch, bias=False),\n",
        "            nn.BatchNorm2d(in_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # Pointwise\n",
        "            nn.Conv2d(in_ch, out_ch, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.blocks(x)\n",
        "        x = self.pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "frontend = LogMelSpec().to(CFG['device'])\n",
        "model = DSCNN(num_classes=len(CLASSES)).to(CFG['device'])\n",
        "\n",
        "print(f\"Total Number of Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d21f237c16c94994b9176b480ab67a57",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "0a6-ntNsM5ua"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cell_id": "d9e27613a4d24de8af96252759229e6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "execution_context_id": "5a0b763e-8f84-45e7-80a9-b4378bb88e17",
        "execution_millis": 565056,
        "execution_start": 1767783671523,
        "id": "wFasa2eIM5ua",
        "outputId": "841d0e9e-fece-4b6e-8829-ad523cd0b93b",
        "source_hash": "4825ed50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Starting Training...\n",
            "Epoch  5/100 | Val Acc: 78.50% (Best: 78.50%)\n",
            "Epoch 10/100 | Val Acc: 94.50% (Best: 94.50%)\n",
            "Epoch 15/100 | Val Acc: 85.00% (Best: 95.50%)\n",
            "Epoch 20/100 | Val Acc: 97.00% (Best: 97.00%)\n",
            "Epoch 25/100 | Val Acc: 96.50% (Best: 97.00%)\n",
            "Epoch 30/100 | Val Acc: 97.50% (Best: 97.50%)\n",
            "Epoch 35/100 | Val Acc: 96.00% (Best: 98.50%)\n",
            "Epoch 40/100 | Val Acc: 96.50% (Best: 98.50%)\n",
            "Epoch 45/100 | Val Acc: 97.00% (Best: 98.50%)\n",
            "Epoch 50/100 | Val Acc: 96.50% (Best: 98.50%)\n",
            "Epoch 55/100 | Val Acc: 98.00% (Best: 98.50%)\n",
            "Epoch 60/100 | Val Acc: 97.50% (Best: 98.50%)\n",
            "Epoch 65/100 | Val Acc: 98.00% (Best: 98.50%)\n",
            "Epoch 70/100 | Val Acc: 98.00% (Best: 98.50%)\n",
            "Epoch 75/100 | Val Acc: 98.50% (Best: 98.50%)\n",
            "Epoch 80/100 | Val Acc: 98.00% (Best: 99.00%)\n",
            "Epoch 85/100 | Val Acc: 98.00% (Best: 99.00%)\n",
            "Epoch 90/100 | Val Acc: 98.00% (Best: 99.00%)\n",
            "Epoch 95/100 | Val Acc: 98.00% (Best: 99.00%)\n",
            "Epoch 100/100 | Val Acc: 98.00% (Best: 99.00%)\n",
            "\n",
            "Evaluating on Test Set...\n",
            "Final Test Accuracy: 100.00%\n",
            "Accuracy requirement met (> 99.4%)\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=CFG['lr'], weight_decay=1e-3)\n",
        "# OneCycleLR for convergence\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, max_lr=CFG['lr'],\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=CFG['epochs']\n",
        ")\n",
        "\n",
        "best_acc = 0.0\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "if len(train_ds) > 0: # Only train if data is present\n",
        "    print(\"[INFO] Starting Training...\")\n",
        "    for epoch in range(CFG['epochs']):\n",
        "        # Train\n",
        "        model.train()\n",
        "        for wavs, labels in train_loader:\n",
        "            wavs, labels = wavs.to(CFG['device']), labels.to(CFG['device'])\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Apply Augmentations in Time Domain\n",
        "                wavs = augment(wavs)\n",
        "                features = frontend(wavs)\n",
        "\n",
        "            # Freq Masking\n",
        "            if random.random() < 0.3:\n",
        "                 features = T.FrequencyMasking(freq_mask_param=10)(features)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step() # Step per batch for OneCycleLR\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for wavs, labels in val_loader:\n",
        "                wavs, labels = wavs.to(CFG['device']), labels.to(CFG['device'])\n",
        "                features = frontend(wavs)\n",
        "                outputs = model(features)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1:2d}/{CFG['epochs']} | Val Acc: {val_acc:.2f}% (Best: {best_acc:.2f}%)\")\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: No training data found. Skipping training loop.\")\n",
        "\n",
        "\n",
        "# Test Accuracy\n",
        "if len(test_ds) > 0:\n",
        "    print(\"\\nEvaluating on Test Set...\")\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for wavs, labels in test_loader:\n",
        "            wavs, labels = wavs.to(CFG['device']), labels.to(CFG['device'])\n",
        "            features = frontend(wavs)\n",
        "            outputs = model(features)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_acc = 100 * correct / total\n",
        "    print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    if test_acc > 99.4:\n",
        "        print(\"Accuracy requirement met (> 99.4%)\")\n",
        "    else:\n",
        "        print(f\"Accuracy is below target ({test_acc:.2f}% vs 99.4%). Consider increasing epochs or model capacity.\")\n",
        "else:\n",
        "    print(\"No test data present.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bb437cc2d6e14c8e97282803f626d438",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "hFOMimGJM5ua"
      },
      "source": [
        "### Export, Quantization, Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "143f247659944fd68b15dc29e65b8535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "execution_context_id": "5a0b763e-8f84-45e7-80a9-b4378bb88e17",
        "execution_millis": 11437,
        "execution_start": 1767784290069,
        "id": "67uaTAx1M5ua",
        "outputId": "457e0a06-5b17-4398-b764-cea88ea67851",
        "source_hash": "9f5e1a65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Exporting : \n",
            "[INFO] Saving files to: /tmp\n",
            "[torch.onnx] Obtain model graph for `LogMelSpec([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `LogMelSpec([...]` with `torch.export.export(..., strict=False)`... ✅\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ✅\n",
            "[torch.onnx] Translate the graph into ONNX...\n",
            "[torch.onnx] Translate the graph into ONNX... ✅\n",
            "Applied 1 of general pattern rewrite rules.\n",
            "Frontend Exported.\n",
            "[torch.onnx] Obtain model graph for `DSCNN([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `DSCNN([...]` with `torch.export.export(..., strict=False)`... ✅\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ✅\n",
            "[torch.onnx] Translate the graph into ONNX...\n",
            "[torch.onnx] Translate the graph into ONNX... ✅\n",
            "Applied 22 of general pattern rewrite rules.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backbone Exported.\n",
            "Starting Quantization Process...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Files are ready at: /tmp:\n",
            " - Frontend: 62.04 KB\n",
            " - Model:    55.44 KB\n",
            "----------------------------------------------\n",
            "Total Size:  117.48 KB\n",
            "Files copied to current directory: /content\n",
            "Submission is under 300 KB.\n",
            "\n",
            "Moving files from /tmp to /work...\n",
            "Checking /tmp Content\n",
            "Group1_model.onnx -> Copied to workspace.\n",
            "Group1_frontend.onnx -> Copied to workspace.\n",
            "Group1_model_float32.onnx -> Copied to workspace.\n",
            "\n",
            " Process Complete! 3 model files should be visible in the file explorer.\n"
          ]
        }
      ],
      "source": [
        "# Bind frontend and backbone for export\n",
        "model.frontend = frontend\n",
        "\n",
        "print(f\"\\nExporting : \")\n",
        "\n",
        "import shutil\n",
        "\n",
        "OUTPUT_DIR = \"/tmp\"\n",
        "if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "frontend_fname = os.path.join(OUTPUT_DIR, \"Group1_frontend.onnx\")\n",
        "model_fp32_fname = os.path.join(OUTPUT_DIR, \"Group1_model_float32.onnx\")\n",
        "model_int8_fname = os.path.join(OUTPUT_DIR, \"Group1_model.onnx\")\n",
        "\n",
        "print(f\"[INFO] Saving files to: {OUTPUT_DIR}\")\n",
        "\n",
        "device_cpu = torch.device(\"cpu\")\n",
        "model.to(device_cpu)\n",
        "model.eval()\n",
        "\n",
        "# Dummy input :\n",
        "dummy_input = torch.randn(1, 1, CFG['sampling_rate']).to(device_cpu)\n",
        "\n",
        "#1) Frontend Export\n",
        "try:\n",
        "    torch.onnx.export(\n",
        "        model.frontend, dummy_input, frontend_fname,\n",
        "        input_names=['input'], output_names=['output'],\n",
        "        dynamo=True, optimize=True, report=False, external_data=False\n",
        "    )\n",
        "    time.sleep(2)\n",
        "    if not os.path.exists(frontend_fname): raise FileNotFoundError(f\"Frontend not found at {frontend_fname}\")\n",
        "    print(\"Frontend Exported.\")\n",
        "except Exception as e:\n",
        "    print(f\"Frontend Export Error: {e}\")\n",
        "\n",
        "#2) Backbone Export\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        # Frontend to Backbone\n",
        "        frontend_out = model.frontend(dummy_input)\n",
        "        if frontend_out.dim() == 3: frontend_out = frontend_out.unsqueeze(1)\n",
        "\n",
        "    torch.onnx.export(\n",
        "        model.backbone, frontend_out, model_fp32_fname,\n",
        "        input_names=['input'], output_names=['output'],\n",
        "        dynamo=True, optimize=True, report=False, external_data=False\n",
        "    )\n",
        "    time.sleep(2)\n",
        "    if not os.path.exists(model_fp32_fname): raise FileNotFoundError(f\"Backbone not found at {model_fp32_fname}\")\n",
        "    print(\"Backbone Exported.\")\n",
        "except Exception as e:\n",
        "    print(f\"Backbone Export Error: {e}\")\n",
        "\n",
        "#3) Quantization\n",
        "class DataReader(CalibrationDataReader):\n",
        "    def __init__(self, dataset, frontend_path):\n",
        "        self.dataset = dataset\n",
        "        self.ort_frontend = ort.InferenceSession(frontend_path, providers=['CPUExecutionProvider'])\n",
        "        self.enum_data = None\n",
        "    def get_next(self):\n",
        "        if self.enum_data is None: self.enum_data = iter(self.dataset)\n",
        "        batch = next(self.enum_data, None)\n",
        "        if batch is None: return None\n",
        "        x, _ = batch\n",
        "        if isinstance(x, torch.Tensor): x = x.detach().cpu().numpy()\n",
        "        if x.ndim == 1: x = np.expand_dims(x, 0)\n",
        "        if x.ndim == 2: x = np.expand_dims(x, 0)\n",
        "        frontend_in = self.ort_frontend.get_inputs()[0].name\n",
        "        feats = self.ort_frontend.run(None, {frontend_in: x})[0]\n",
        "        # Backbone size:\n",
        "        if len(feats.shape) == 3: feats = np.expand_dims(feats, 1)\n",
        "        return {'input': feats}\n",
        "    def rewind(self): self.enum_data = None\n",
        "\n",
        "if os.path.exists(model_fp32_fname) and os.path.exists(frontend_fname):\n",
        "    print(\"Starting Quantization Process...\")\n",
        "    dr = DataReader(val_ds, frontend_fname)\n",
        "    q_config = StaticQuantConfig(dr, quant_format=QuantFormat.QDQ, calibrate_method=CalibrationMethod.MinMax, activation_type=QuantType.QInt8, weight_type=QuantType.QInt8, per_channel=False)\n",
        "    quantize(model_fp32_fname, model_int8_fname, q_config)\n",
        "\n",
        "    f_size = os.path.getsize(frontend_fname) / 1024\n",
        "    m_size = os.path.getsize(model_int8_fname) / 1024\n",
        "    t_size = f_size + m_size\n",
        "\n",
        "\n",
        "    print(f\"\\nFiles are ready at: {OUTPUT_DIR}:\")\n",
        "    print(f\" - Frontend: {f_size:.2f} KB\")\n",
        "    print(f\" - Model:    {m_size:.2f} KB\")\n",
        "    print(f\"----------------------------------------------\")\n",
        "    print(f\"Total Size:  {t_size:.2f} KB\")\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        current_dir = os.getcwd()\n",
        "        shutil.copy(frontend_fname, os.path.join(current_dir, \"Group1_frontend.onnx\"))\n",
        "        shutil.copy(model_int8_fname, os.path.join(current_dir, \"Group1_model.onnx\"))\n",
        "        print(f\"Files copied to current directory: {current_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not copy to current dir (permissions?), but files are safe in {OUTPUT_DIR}\")\n",
        "\n",
        "    if t_size < 300:\n",
        "        print(f\"Submission is under 300 KB.\")\n",
        "    else:\n",
        "        print(f\"Warning: Total size is {t_size:.2f} KB.\")\n",
        "else:\n",
        "    print(\"Error: Export files missing.\")\n",
        "\n",
        "# File\n",
        "source_dir = '/tmp'    # Source directory\n",
        "target_dir = '/work'   # Target directory\n",
        "\n",
        "print(f\"\\nMoving files from {source_dir} to {target_dir}...\")\n",
        "\n",
        "# Ensure target directory exists\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "# Move ONNX files\n",
        "files_found = 0\n",
        "if os.path.exists(source_dir):\n",
        "    print(f\"Checking {source_dir} Content\")\n",
        "    for file_name in os.listdir(source_dir):\n",
        "        # Filter for .onnx files\n",
        "        if file_name.endswith('.onnx'):\n",
        "            full_source_path = os.path.join(source_dir, file_name)\n",
        "            full_target_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "            try:\n",
        "                shutil.copy(full_source_path, full_target_path)\n",
        "                print(f\"{file_name} -> Copied to workspace.\")\n",
        "                files_found += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Could not copy {file_name}. Reason: {e}\")\n",
        "\n",
        "    if files_found == 0:\n",
        "        print(\"WARNING: No .onnx files found in /tmp. Export might have failed.\")\n",
        "    else:\n",
        "        print(f\"\\n Process Complete! {files_found} model files should be visible in the file explorer.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Error: Source directory ({source_dir}) not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "d76360d6be3e4cefb5460fd8e86040ae",
        "deepnote_cell_type": "code",
        "id": "DSS2UEoaM5ua"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "KjrlDACMM5ub"
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=154c2188-6af2-4819-9ca5-4f23502d7a8f' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "deepnote_notebook_id": "5dbe037327ba41dbb44e830cb52531dd",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
